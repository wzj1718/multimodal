import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoImageProcessor  
from transformers import AutoModelForCausalLM
from transformers import AutoModelForVision2Seq,AutoProcessor
import shutil
import os
import shutil
import torch
from transformers import AutoProcessor, AutoTokenizer

def _blend_linear_(target_linear, source_linear, alpha_vl=0.9, alpha_base=0.1):
    """
    åªèåˆ Linear çš„ weightï¼ˆä»¥åŠ bias è‹¥å­˜åœ¨ï¼‰
    target <- alpha_vl * target + alpha_base * source
    """
    with torch.no_grad():
        # weight
        tw = target_linear.weight
        sw = source_linear.weight.to(dtype=tw.dtype, device=tw.device)
        if tw.shape != sw.shape:
            raise ValueError(f"weight shape mismatch: tgt={tw.shape}, src={sw.shape}")
        tw.copy_(alpha_vl * tw + alpha_base * sw)

        # bias (optional)
        tb = getattr(target_linear, "bias", None)
        sb = getattr(source_linear, "bias", None)
        if tb is not None:
            if sb is None:
                raise ValueError("target has bias but source has no bias")
            sb = sb.to(dtype=tb.dtype, device=tb.device)
            if tb.shape != sb.shape:
                raise ValueError(f"bias shape mismatch: tgt={tb.shape}, src={sb.shape}")
            tb.copy_(alpha_vl * tb + alpha_base * sb)


def replace_self_attn_proj_from_base_model(
    vl_model,
    base_model,
    start_layer=24,
    end_layer=35,
    save_dir="./merged_qwen3vl",
    orig_vl_model_path=None,   # å»ºè®®ä¼  Qwen3-VL çš„åŸå§‹ç›®å½•
    alpha_vl=0.9,
    alpha_base=0.1,
):
    """
    åªèåˆ VL æ¨¡å‹æŒ‡å®šå±‚ self_attn çš„ q/k/v/o æŠ•å½±ï¼š
        W_new = alpha_vl * W_vl + alpha_base * W_base

    ä¿æŒ VL çš„ q_norm/k_norm ä¸å˜ï¼›MLP å®Œå…¨ä¸åŠ¨ã€‚
    """
    # 1) æ‹¿åˆ°è¯­è¨€å±‚
    vl_layers = vl_model.model.language_model.layers
    base_layers = base_model.model.layers

    assert len(vl_layers) == len(base_layers), \
        f"âŒ å±‚æ•°ä¸åŒ¹é…ï¼šVLæœ‰{len(vl_layers)}å±‚ï¼ŒBaseæœ‰{len(base_layers)}å±‚"

    # å¯é€‰ï¼šé¿å…ä½ æ‰‹æ»‘æŠŠ alpha è®¾é”™
    if abs((alpha_vl + alpha_base) - 1.0) > 1e-6:
        raise ValueError(f"alpha_vl + alpha_base should be 1.0, got {alpha_vl + alpha_base}")

    print(f"ğŸ”§ å¼€å§‹èåˆå±‚ {start_layer}~{end_layer} çš„ self_attn æŠ•å½±(q/k/v/o)...")
    print(f"ğŸ“Š æ€»å±‚æ•°: {len(vl_layers)}")
    blended_layers = []

    with torch.no_grad():
        for i in range(start_layer, end_layer + 1):
            vl_attn = vl_layers[i].self_attn
            base_attn = base_layers[i].self_attn

            # 2) åªèåˆ q/k/v/o
            for name in ["q_proj", "k_proj", "v_proj", "o_proj"]:
                if not (hasattr(vl_attn, name) and hasattr(base_attn, name)):
                    raise AttributeError(f"{name} ä¸å­˜åœ¨äº self_attn ä¸­ (layer={i})")
                v_mod = getattr(vl_attn, name)
                b_mod = getattr(base_attn, name)
                _blend_linear_(v_mod, b_mod, alpha_vl=alpha_vl, alpha_base=alpha_base)

            # 3) æ˜ç¡®å£°æ˜ï¼šq_norm / k_norm ä¸åŠ¨ï¼ˆè¿™é‡Œä¸éœ€è¦å†™ä»£ç ï¼Œä»€ä¹ˆéƒ½ä¸åšå°±æ˜¯â€œä¸åŠ¨â€ï¼‰
            blended_layers.append(i)

    print(f"ğŸ¯ æˆåŠŸèåˆ {len(blended_layers)} å±‚ï¼š{blended_layers}")

    # === ä¿å­˜æ¨¡å‹ ===
    os.makedirs(save_dir, exist_ok=True)
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹æƒé‡åˆ°ï¼š{save_dir}")
    vl_model.save_pretrained(save_dir)
    print("âœ… æ¨¡å‹æƒé‡ä¿å­˜å®Œæˆï¼")

    # === åŒæ­¥ä¿å­˜ tokenizer / processor / chat_template ===
    if orig_vl_model_path is not None:
        print("ğŸ“¦ æ­£åœ¨å¤åˆ¶ tokenizer / processor / chat_template.json ...")
        processor = AutoProcessor.from_pretrained(orig_vl_model_path)
        tokenizer = AutoTokenizer.from_pretrained(orig_vl_model_path)
        processor.save_pretrained(save_dir)
        tokenizer.save_pretrained(save_dir)

        src_template = os.path.join(orig_vl_model_path, "chat_template.json")
        dst_template = os.path.join(save_dir, "chat_template.json")
        if os.path.exists(src_template):
            shutil.copy(src_template, dst_template)
            print(f"âœ… å·²å¤åˆ¶ chat_template.json åˆ° {dst_template}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° chat_template.jsonï¼ˆä¸å½±å“ä¿å­˜æƒé‡ï¼Œä½†å¯èƒ½å½±å“æŸäº› chat æ¨¡æ¿æ¨ç†ï¼‰")
    else:
        print("âš ï¸ æœªæä¾› orig_vl_model_pathï¼šå·²ä»…ä¿å­˜æ¨¡å‹æƒé‡ã€‚è‹¥è¦ç›´æ¥æ¨ç†ï¼Œè¯·å¦å¤–ä¿å­˜ processor/tokenizerã€‚")

    print(f"ğŸ‰ æ¨¡å‹èåˆä¸ä¿å­˜å…¨éƒ¨å®Œæˆï¼š{save_dir}")
    return vl_model


qwen_vl_path = "/dss/dssfs04/lwp-dss-0002/pn25ho/pn25ho-dss-0001/di93pux/multimodal/models/Qwen3-VL-4B-Instruct"
qwen_base_path = "/dss/dssfs04/lwp-dss-0002/pn25ho/pn25ho-dss-0001/di93pux/multimodal/models/Qwen3-4B"

base_save_root = "/dss/dssfs04/lwp-dss-0002/pn25ho/pn25ho-dss-0001/di93pux/multimodal/merged_models/merge_4B_only_qkvo/19"

start_layers = list(range(19, 29))  # 19 ~ 28
end_layer = 35

alpha_vl = 0.9
alpha_base = 0.1

print("ğŸš€ æ­£åœ¨åŠ è½½ base modelï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰...")
base_model = AutoModelForCausalLM.from_pretrained(
    qwen_base_path,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="cpu",
)

for start_layer in start_layers:
    save_path = (
        f"{base_save_root}/"
        f"merge_{start_layer}--{end_layer}+{alpha_base}base+{alpha_vl}vl"
    )

    print(f"\nğŸ” å¼€å§‹å¤„ç† start_layer={start_layer}, end_layer={end_layer}")
    print(f"ğŸ’¾ ä¿å­˜è·¯å¾„ï¼š{save_path}")

    # âš ï¸ æ¯æ¬¡éƒ½é‡æ–°åŠ è½½ VL æ¨¡å‹ï¼Œä¿è¯å®éªŒç‹¬ç«‹
    vl_model = AutoModelForVision2Seq.from_pretrained(
        qwen_vl_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="cpu",
    )

    replace_self_attn_proj_from_base_model(
        vl_model=vl_model,
        base_model=base_model,
        start_layer=start_layer,
        end_layer=end_layer,
        save_dir=save_path,
        orig_vl_model_path=qwen_vl_path,
        alpha_vl=alpha_vl,
        alpha_base=alpha_base,
    )

    # æ˜¾å¼é‡Šæ”¾ï¼ˆCPU å†…å­˜ä¹Ÿä¸å°ï¼‰
    del vl_model
    torch.cuda.empty_cache()

print("\nğŸ‰ æ‰€æœ‰ start_layer âˆˆ [19, 27] çš„ merge å®éªŒå·²å®Œæˆï¼")




